---
title: "Final Project"
author: "Sara Culhane and Brenna Sullivan"
date: "December 22, 2017"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    df_print: kable
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
---




## Load all packages

```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
require(mosaic)   # Load additional packages here 
library(ggplot2)
library(readr)

# Some customization.  You can alter or delete as desired (if you know what you are doing).
trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```



## Load data and perform data cleaning

Please delete all "notes" before submission.

Note: CSV/data files should be read assuming they are in the `data` folder. In
other words, load data via `read_csv("data/CSV_NAME.csv")` and not via
`read_csv("/Users/aykim/Documents/MATH495/Final_Project/data/CSV_NAME.csv")`

Kaggle Competition: House Prices: Advanced Regression Techniques (Scoring Mechinism: RMSE)
https://www.kaggle.com/c/house-prices-advanced-regression-techniques

```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
train1 <- read.csv("data/FinalProjectTrain.csv", sep=",", stringsAsFactors = F)
kaggle_test <- read.csv("data/FinalProjectTest.csv") #predict SalePrice for kaggle submission
```

```{r}
#data cleaning

#RoofStyle
train1$RoofStyle[train1$RoofStyle == "Flat"] <- "Other"
train1$RoofStyle[train1$RoofStyle == "Gambrel"] <- "Other"
train1$RoofStyle[train1$RoofStyle == "Mansard"] <- "Other"
train1$RoofStyle[train1$RoofStyle == "Shed"] <- "Other"

#Exterior1st
train1$Exterior1st[train1$Exterior1st == "AsphShn"] <- "Other"
train1$Exterior1st[train1$Exterior1st == "BrkComm"] <- "Other"
train1$Exterior1st[train1$Exterior1st == "Stone"] <- "Other"

#heating
train1$Heating[train1$Heating == "Floor"] <- "Other"
train1$Heating[train1$Heating == "Grav"] <- "Other"
train1$Heating[train1$Heating == "OthW"] <- "Other"
train1$Heating[train1$Heating == "Wall"] <- "Other"

#Condition1
train1$Condition1[train1$Condition1 == "RRNn"] <- "Other"
train1$Condition1[train1$Condition1 == "RRNe"] <- "Other"
train1$Condition1[train1$Condition1 == "RRAe"] <- "Other"
train1$Condition1[train1$Condition1 == "Pos8"] <- "Other"

#Condition2
train1$Condition2[train1$Condition2 == "Artery"] <- "Other"
train1$Condition2[train1$Condition2 == "Feedr"] <- "Other"
train1$Condition2[train1$Condition2 == "PosA"] <- "Other"
train1$Condition2[train1$Condition2 == "PosN"] <- "Other"
train1$Condition2[train1$Condition2 == "RRAe"] <- "Other"
train1$Condition2[train1$Condition2 == "RRAn"] <- "Other"
train1$Condition2[train1$Condition2 == "RRNn"] <- "Other"

#Functional
train1$Functional[train1$Functional == "Sev"] <- "Other"
train1$Functional[train1$Functional == "Maj2"] <- "Other"

#ExternalCond
train1$ExterCond[train1$ExterCond == "Fa"] <- "Bad"
train1$ExterCond[train1$ExterCond == "Po"] <- "Bad"
train1$ExterCond[train1$ExterCond == "Ex"] <- "Good"
train1$ExterCond[train1$ExterCond == "Gd"] <- "Good"

#HeatingQC
train1$HeatingQC[train1$HeatingQC == "Po"] <- "Fa"

#SaleType
train1$SaleType[train1$SaleType == "COD"] <- "Other"
train1$SaleType[train1$SaleType == "Con"] <- "Other"
train1$SaleType[train1$SaleType == "ConLD"] <- "Other"
train1$SaleType[train1$SaleType == "ConLI"] <- "Other"
train1$SaleType[train1$SaleType == "ConLw"] <- "Other"
train1$SaleType[train1$SaleType == "CWD"] <- "Other"
train1$SaleType[train1$SaleType == "Oth"] <- "Other"

#RoofStyle
kaggle_test$RoofStyle[kaggle_test$RoofStyle == "Flat"] <- "Other"
kaggle_test$RoofStyle[kaggle_test$RoofStyle == "Gambrel"] <- "Other"
kaggle_test$RoofStyle[kaggle_test$RoofStyle == "Mansard"] <- "Other"
kaggle_test$RoofStyle[kaggle_test$RoofStyle == "Shed"] <- "Other"

#Exterior1st
kaggle_test$Exterior1st[kaggle_test$Exterior1st == "AsphShn"] <- "Other"
kaggle_test$Exterior1st[kaggle_test$Exterior1st == "BrkComm"] <- "Other"
kaggle_test$Exterior1st[kaggle_test$Exterior1st == "CBlock"] <- "Other"
kaggle_test$Exterior1st[kaggle_test$Exterior1st == "Stone"] <- "Other"
kaggle_test$Exterior1st[kaggle_test$Exterior1st == "<NA>"] <- "Other"

#heating
kaggle_test$Heating[kaggle_test$Heating == "Floor"] <- "Other"
kaggle_test$Heating[kaggle_test$Heating == "Grav"] <- "Other"
kaggle_test$Heating[kaggle_test$Heating == "OthW"] <- "Other"
kaggle_test$Heating[kaggle_test$Heating == "Wall"] <- "Other"

#Condition1
kaggle_test$Condition1[kaggle_test$Condition1 == "RRNn"] <- "Other"
kaggle_test$Condition1[kaggle_test$Condition1 == "RRNe"] <- "Other"
kaggle_test$Condition1[kaggle_test$Condition1 == "RRAe"] <- "Other"
kaggle_test$Condition1[kaggle_test$Condition1 == "Pos8"] <- "Other"

#Condition2
kaggle_test$Condition2[kaggle_test$Condition2 == "Artery"] <- "Other"
kaggle_test$Condition2[kaggle_test$Condition2 == "Feedr"] <- "Other"
kaggle_test$Condition2[kaggle_test$Condition2 == "PosA"] <- "Other"
kaggle_test$Condition2[kaggle_test$Condition2 == "PosN"] <- "Other"
kaggle_test$Condition2[kaggle_test$Condition2 == "RRAe"] <- "Other"
kaggle_test$Condition2[kaggle_test$Condition2 == "RRAn"] <- "Other"
kaggle_test$Condition2[kaggle_test$Condition2 == "RRNn"] <- "Other"

#Functional
kaggle_test$Functional[kaggle_test$Functional == "Sev"] <- "Other"
kaggle_test$Functional[kaggle_test$Functional == "Maj2"] <- "Other"

#ExternalCond
kaggle_test$ExterCond[kaggle_test$ExterCond == "Fa"] <- "Bad"
kaggle_test$ExterCond[kaggle_test$ExterCond == "Po"] <- "Bad"
kaggle_test$ExterCond[kaggle_test$ExterCond == "Ex"] <- "Good"
kaggle_test$ExterCond[kaggle_test$ExterCond == "Gd"] <- "Good"

#HeatingQC
kaggle_test$HeatingQC[kaggle_test$HeatingQC == "Po"] <- "Fa"

#SaleType
kaggle_test$SaleType[kaggle_test$SaleType == "COD"] <- "Other"
kaggle_test$SaleType[kaggle_test$SaleType == "Con"] <- "Other"
kaggle_test$SaleType[kaggle_test$SaleType == "ConLD"] <- "Other"
kaggle_test$SaleType[kaggle_test$SaleType == "ConLI"] <- "Other"
kaggle_test$SaleType[kaggle_test$SaleType == "ConLw"] <- "Other"
kaggle_test$SaleType[kaggle_test$SaleType == "CWD"] <- "Other"
kaggle_test$SaleType[kaggle_test$SaleType == "Oth"] <- "Other"

```

```{r}
#psuedo train and test set
set.seed(1)
train_id <- sample(1:dim(train1)[1], dim(train1)[1]/2, replace = FALSE)
train <- train1[train_id, ] #data to fit the model to
test <- train1[-train_id, ] #data to CV on
```


```{r}
#Drop factor variables with less than 2 levels & keep non-factor vars from Kaggle Competition discusstion(https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/24399)

names(train) <- make.names(names(train))
names(train1) <- make.names(names(train1))
names(kaggle_test) <- make.names(names(kaggle_test))
features <- setdiff(colnames(train), c("Id", "SalePrice"))
for (f in features) {
  if (any(is.na(train[[f]]))) 
    if (is.character(train[[f]])){ 
      train[[f]][is.na(train[[f]])] <- "Others"
    }else{
      train[[f]][is.na(train[[f]])] <- -999  
    }
}
features <- setdiff(colnames(train1), c("Id", "SalePrice"))
for (f in features) {
  if (any(is.na(train1[[f]]))) 
    if (is.character(train1[[f]])){ 
      train1[[f]][is.na(train1[[f]])] <- "Others"
    }else{
      train1[[f]][is.na(train1[[f]])] <- -999  
    }
}
features <- setdiff(colnames(kaggle_test), c("Id", "SalePrice"))
for (f in features) {
  if (any(is.na(kaggle_test[[f]]))) 
    if (is.character(kaggle_test[[f]])){ 
      kaggle_test[[f]][is.na(kaggle_test[[f]])] <- "Others"
    }else{
      kaggle_test[[f]][is.na(kaggle_test[[f]])] <- -999  
    }
}
column_class <- lapply(train,class)
column_class <- column_class[column_class != "factor"]
factor_levels <- lapply(train, nlevels)
factor_levels <- factor_levels[factor_levels > 1]
column_class2 <- lapply(train1,class)
column_class2 <- column_class[column_class != "factor"]
factor_levels2 <- lapply(train1, nlevels)
factor_levels2 <- factor_levels[factor_levels > 1]
column_class3 <- lapply(kaggle_test,class)
column_class3 <- column_class[column_class != "factor"]
factor_levels3 <- lapply(kaggle_test, nlevels)
factor_levels3 <- factor_levels[factor_levels > 1]
train <- train[,names(train) %in% c(names(factor_levels), names(column_class))]
test <- train[,names(train) %in% c(names(factor_levels), names(column_class))]

train1 <- train1[,names(train1) %in% c(names(factor_levels2), names(column_class2))]
kaggle_test <- kaggle_test[,names(kaggle_test) %in% c(names(factor_levels3), names(column_class3))]

train <- as.data.frame(unclass(train))
test <- as.data.frame(unclass(test))
train1 <- as.data.frame(unclass(train1))
kaggle_test <- as.data.frame(unclass(kaggle_test))

#Train model
lmfull <- lm(log(SalePrice) ~ . , data=train)
```

## EDA visualizations and tables

Note: If you had to illustrate using no modelling but only graphs and tables which
variables have the most predictive power, which would you include?

* Perform a cross-validation on only the final/ultimate model used for your
submission.
* The "score" in question should be the same as used to compute the Kaggle
leaderboard. In other words, your estimated score should be roughly equal to the
score returned by Kaggle after your submission.

Predicting Sale Price (in USD) of houses with no modeling:

```{r}
histogram(train$SalePrice, type = "percent", xlab= "Sale Price (USD)", ylab= "Percent of Homes", main = "Sale Price of Homes in Ames, Iowa") #skewed right, not normal
bwplot(train$SalePrice, xlab= "Sale Price (USD)", main = "Log of Sale Price of Homes in Ames, Iowa")
histogram(log(train$SalePrice), type = "percent", xlab = "Log of Sale Price (USD)", ylab = "Percent of Homes", main = "Sale Price of Homes in Ames, Iowa") #much more normal when logSalePrice is used
bwplot(log(train$SalePrice), xlab = "Log of Sale Price (USD)", main = "Log of Sale Price of Homes in Ames, Iowa")
mean(train$SalePrice)
mean(log(train$SalePrice))
```

Our Hypothesis for Important Variables:

```{r}
#LotArea (Quantitative)

xyplot(log(SalePrice) ~ LotArea, data=train, type = c("p", "r"), xlab = "Lot Area (Sq. Feet)", ylab= "Log of Sale Price", main = "Log of Sale Price vs. Lot Area") #Highly influenced by outliers

#Overall Quality (Categorical)

boxplot(log(SalePrice) ~ OverallQual, data=train, xlab = "Quality Rating (1-10 Scale)", ylab = "Log of Sale Price", main = "Log of Sale Price vs. Overall Quality")
```



Predicting Sale Price (in USD) of houses with modeling:

###1. Subset Selection (Forward or Backwards selection of predictors (Stat230))



```{r}
train$log_saleprice <- log(train$SalePrice)
```

```{r}
model_formula <- as.formula("log_saleprice ~ Id + MSSubClass + MSZoning + LotFrontage + LotArea + Street + Alley + LotShape + LandContour + LotConfig + LandSlope + Neighborhood + Condition1 + Condition2+ BldgType + HouseStyle + OverallQual + OverallCond + YearBuilt +YearRemodAdd + RoofStyle + RoofMatl + Exterior1st + Exterior2nd + MasVnrType + MasVnrArea + ExterQual + ExterCond + Foundation + BsmtCond + BsmtExposure + BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 + BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + Heating + HeatingQC + CentralAir + Electrical + X1stFlrSF + X2ndFlrSF + LowQualFinSF + GrLivArea + BsmtFullBath +BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + FireplaceQu + GarageType + GarageYrBlt + GarageFinish + GarageCars + GarageArea + GarageQual + PavedDrive + WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch + PoolArea + PoolQC + Fence + MiscFeature + MiscVal + MoSold + YrSold + SaleType + SaleCondition")
```

```{r}
lmfull <- lm(model_formula, data=train)

lmempty <- lm(log_saleprice ~ 1, data=train) # model with only intercept
model1 <- step(lmempty, scope=list(lower=lmempty, upper=lmfull), direction="forward")
summary(model1)

test$log_saleprice <- log(test$SalePrice)

test_preds <- predict(model1, test)

diff <- test_preds - test$log_saleprice

RMSE <- function(diff) {
  RMSE <- sqrt(mean(diff^2))
  return(RMSE)
}
RMSE(diff)

```


```{r}
train1$log_saleprice <- log(train1$SalePrice)
lmfull2 <- lm(log_saleprice ~ . , data=train1)

lmempty2 <- lm(log_saleprice ~ 1, data=train1)

model2 <- lm(formula = log_saleprice ~ OverallQual + Neighborhood + GrLivArea + 
    BsmtFinSF1 + YearRemodAdd + TotalBsmtSF + GarageQual + OverallCond + 
    CentralAir + SaleCondition + GarageCars + LotArea + YearBuilt + 
    BldgType + MSZoning + Functional + BsmtFullBath + KitchenQual + 
    Condition1 + WoodDeckSF + FireplaceQu + BsmtFinSF2 + ScreenPorch + 
    FullBath + LotConfig + Heating + HeatingQC + BsmtExposure + 
    X3SsnPorch + ExterCond + Street + SaleType + EnclosedPorch + 
    HalfBath, data = train)

preds <- predict(model2,kaggle_test)


kaggle_test$log_saleprice <- 0
kaggle_test$SalePrice <- 0
kaggle_preds <- predict(model2,kaggle_test)

```


###2. Dimension Reduction (PCA)

### PCA using top quantative 

```{r}
library(factoextra)
pca <- train %>% 
  select(GrLivArea,GarageArea,LotFrontage, OverallQual,OverallCond,BsmtFinSF1,BsmtFinSF2,TotalBsmtSF,TotRmsAbvGrd,PoolArea,YearBuilt,YearRemodAdd)
pca_comp <- prcomp(pca,scale=FALSE)
fviz_eig(pca_comp)

fviz_pca_ind(pca_comp,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

fviz_pca_var(pca_comp,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

p <- lm(log_saleprice ~ GrLivArea+ GarageArea+LotFrontage+ OverallQual+OverallCond+BsmtFinSF1+BsmtFinSF2+TotalBsmtSF+TotRmsAbvGrd+PoolArea+YearBuilt+YearRemodAdd ,data=train)
summary(p)
p2 <-  lm(log_saleprice ~  GarageArea+LotFrontage+TotalBsmtSF+ OverallQual+OverallCond+TotRmsAbvGrd+PoolArea+YearBuilt+YearRemodAdd,data=train)
y1 <- predict(p,test)
y2 <- predict(p2,test)
r1 <- sqrt(mean((y1-test$log_saleprice)^2))
r2 <- sqrt(mean((y2-test$log_saleprice)^2))

kaggle_test$log_saleprice <- 0
kaggle_test$SalePrice <- 0
p2 <- lm(log_saleprice ~ GrLivArea+ GarageArea+LotFrontage+ OverallQual+OverallCond+BsmtFinSF1+BsmtFinSF2+TotalBsmtSF+TotRmsAbvGrd+PoolArea+YearBuilt+YearRemodAdd ,data=train1)

kag <- predict(p2,kaggle_test)
kag <- exp(kag)
mkag <- mean(kag[1])
kaggle_test$SalePrice <- 0
kaggle_test$SalePrice <- kag
kaggle_test$SalePrice <- ifelse(is.na(kaggle_test$SalePrice), mkag, kag)

kag1 <- kaggle_test %>% select(Id, SalePrice)

write_csv(kag1, "submission2.csv")
```

```{r}
X <- train %>% 
  select(TotalBsmtSF,LotFrontage,YearBuilt)
ggplot(data=X, aes(x=TotalBsmtSF, y=LotFrontage)) +
  geom_text(label=X$YearBuilt) +
  geom_smooth(method="lm", se=FALSE, linetype="dashed", size=0.5, col="black") +
  labs(x="X1: TotalBsmtSF", y="X2: LotFrontage")
```

###3. Shrinkage/ Regularization (LASSO)
```{r}
library(glmnet)
library(tidyr)
get_LASSO_coefficients <- function(LASSO_fit){
  coeff_values <- LASSO_fit %>% 
    broom::tidy() %>% 
    as_tibble() %>% 
    select(-c(step, dev.ratio)) %>% 
    tidyr::complete(lambda, nesting(term), fill = list(estimate = 0)) %>% 
    arrange(desc(lambda)) %>% 
    select(term, estimate, lambda)
  return(coeff_values)
}

mod <- as.formula("log_saleprice ~ SalePrice + Neighborhood + ExterQual + 
    PoolQC + GarageQual + RoofMatl + BsmtQual + OverallQual + 
    Functional + YearRemodAdd + GrLivArea + Exterior1st + CentralAir + 
    GarageType + BsmtFullBath + YearBuilt + OverallCond + FireplaceQu + 
    GarageCars + MSZoning + HouseStyle + BedroomAbvGr + ScreenPorch + 
    Fence + Id + WoodDeckSF + SaleCondition + Alley + ExterCond + 
    Condition2 + GarageFinish + PavedDrive + Heating + MSSubClass + 
    OpenPorchSF + RoofStyle + MoSold + BsmtFinType1 + BsmtCond + 
    Foundation + BsmtHalfBath + MiscVal + LotFrontage")

predictor_matrix <- model.matrix(mod, data=train)[, -1]
predictor_matrix2 <- model.matrix(mod, data=test)[,-1]
lambda_inputs <- 10^seq(-2, 10, length = 100)
predictor_matrix3 <- model.matrix(mod, data=kaggle_test)[,-1]

LASSO_fit <- glmnet(x=predictor_matrix, y=train$log_saleprice, alpha = 1, lambda = lambda_inputs)
LASSO_CV <- cv.glmnet(x=predictor_matrix, y=train$log_saleprice, alpha = 1)
lambda_star_1SE <- LASSO_CV$lambda.1se
# 5. Get beta-hat coefficients for ALL values of knob/tuning parameter lambda
LASSO_coefficients <- get_LASSO_coefficients(LASSO_fit)

ggplot(LASSO_coefficients, aes(x=lambda, y=estimate, col=term)) +
  geom_line() +
  labs(x="lambda", y="beta-hat coefficient estimate")

### Predict for test coeff
y_hat <- predict(LASSO_fit, newx=predictor_matrix2, s=lambda_star_1SE) %>% 
  as.vector()
hist(exp(y_hat))
hist(test$SalePrice, xlab = "y-actual" , main="Historgram of test SalePrice")
RMSE <- sqrt(mean((y_hat - test$log_saleprice)^2))



```


```{r}
plot_LASSO_coefficients <- LASSO_coefficients %>% 
  filter(term != "(Intercept)") %>% 
  ggplot(aes(x=lambda, y=estimate, col=term)) +
  geom_line() +
  scale_x_log10() +
  labs(x="lambda (log10-scale)", y="beta-hat coefficient estimate",
       title="LASSO regularized coefficient for each lambda value")
plot_LASSO_coefficients
```




## Crossvalidation of ultimate model

Note: Hardcode your crossvalidation here i.e. do not use built-in crossvalidation
options.

```{r}
train1$log_saleprice <- log(train1$SalePrice)
kaggle_test$log_saleprice <- runif(nrow(kaggle_test),1,10)
kaggle_test$SalePrice <- 0
```


```{r}
fold <- 10
lambda_inputs <- 10^seq(-2, 10, length = 100)
results <- data_frame(
  lambda = lambda_inputs,
  RMSE = 0
)
for(i in 1:length(lambda_inputs)){
  for(j in 1:10){
    # Set up training & validation (AKA "pretend" test) set based on folds
    train_set <- train %>% 
      filter(fold == j)
     
  
    validation_set <- train %>% 
      filter(fold !=j)
    
    predict <- model.matrix(mod, data=train_set)[, -1]
    
    LASSO_fit <- glmnet(x=predict, y=train_set$log_saleprice, alpha = 1, lambda = lambda_inputs)[1]
    
  }
}
    
```






## Create submission

Note: Output a CSV using `write_csv(DATAFRAME_NAME, path="data/SUBMISSION_NAME.csv")`
that is Kaggle submitable. This submission should return a Kaggle score that is
close to your crossvalidated score.



## Citations and references

Note: All citations and references must be included here.



## Supplementary materials

Note: Anything else you've tried that you'd like to include, but isn't essential to
the above, like other EDA's, other modeling approaches you've tried, etc. Please
set the R code chunk `eval=FALSE` here so that default is that R Markdown
doesn't run the code, but a user can flip this switch if they are curious.


```{r, eval=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}


```





